# INEGI Web Scraper

Sistema de web scraping para extraer informaciÃ³n del sitio oficial del INEGI (Instituto Nacional de EstadÃ­stica y GeografÃ­a de MÃ©xico).

## â° Cron Job AutomÃ¡tico

El sistema ejecuta scraping automÃ¡tico **cada 5 minutos** mediante un scheduler integrado.

## ğŸ—ï¸ Arquitectura de Microservicios

Este proyecto estÃ¡ diseÃ±ado con una **arquitectura modular de microservicios**:

### Servicios Principales:
- **ScraperService** - Servicio de web scraping especializado
- **StorageService** - Servicio de persistencia y almacenamiento
- **SchedulerService** - Servicio de tareas programadas (Cron Job cada 5 min)
- **API REST** - Endpoints para interactuar con los servicios

## ğŸ“‹ DescripciÃ³n

El sistema extrae automÃ¡ticamente:
- TÃ­tulo del sitio
- Secciones principales
- Ãšltimas noticias y comunicados
- Indicadores destacados
- Links importantes

## ğŸš€ InstalaciÃ³n

1. Activa el entorno virtual (si aÃºn no estÃ¡ activado):
```powershell
.\venv\Scripts\Activate.ps1
```

2. Las dependencias ya estÃ¡n instaladas. Si necesitas reinstalarlas:
```powershell
pip install -r requirements.txt
```

## ğŸ“¦ Estructura del Proyecto

```
aiAttack/
â”œâ”€â”€ app.py                      # AplicaciÃ³n principal
â”œâ”€â”€ config.py                   # ConfiguraciÃ³n centralizada
â”œâ”€â”€ scraper.py                  # Scraper legacy (mantener compatibilidad)
â”œâ”€â”€ requirements.txt            # Dependencias
â”‚
â”œâ”€â”€ services/                   # ğŸ”¥ Microservicios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_service.py     # Servicio de web scraping
â”‚   â”œâ”€â”€ storage_service.py     # Servicio de almacenamiento
â”‚   â””â”€â”€ scheduler_service.py   # Servicio de programaciÃ³n
â”‚inmediatamente |
| `/api/data` | GET | Obtener datos en cachÃ© |
| `/api/data/json` | GET | Descargar archivo JSON |
| `/api/data/csv` | GET | Descargar archivo CSV |
| `/api/status` | GET | Estado completo del sistema |
| `/api/files` | GET | Listar archivos de datos |
| `/api/schedule` | POST | Configurar intervalo de scraping
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                       # Datos generados
â”‚   â”œâ”€â”€ inegi_data.json
â”‚   â”œâ”€â”€ inegi_data.csv
â”‚   â””â”€â”€ inegi_latest.json
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_scraper.py
    â””â”€â”€ test_microservices.py  # ğŸ”¥ Test de microservicios
```

## ğŸ”§ Microservicios

### 1. **ScraperService** (`services/scraper_service.py`)
Servicio especializado en web scraping:
- ConexiÃ³n HTTP con headers apropiados
- Parsing de HTML con BeautifulSoup
- ExtracciÃ³n de datos estructurados
- Manejo de errores robusto

### 2. **StorageService** (`services/storage_service.py`)
Servicio de persistencia de datos:
- Guardar en formato JSON
- Guardar en formato CSV
- Cargar datos existentes
- GestiÃ³n de archivos

### 3. **SchedulerService** (`services/scheduler_service.py`)
Servicio de tareas programadas:
- Scraping automÃ¡tico con intervalo configurable
- CachÃ© de datos en memoria
- Control de jobs (start/stop/update)
- Estado del scheduler

### 4. **API REST** (`api/routes.py`)
Endpoints HTTP para interactuar con los servicios:

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/` | GET | InformaciÃ³n de la API |
| `/api/scrape` | GET | Ejecutar scraping ahora |
| `/api/data` | GET | Obtener datos en cachÃ© |
| `/api/data/json` | GET | Descargar archivo JSON |
| `/api/data/csv` | GET | Descargar archivo CSV |
| `/api/status` | GET | Estado del scraper |
| `/api/schedule` | POST | Configurar frecuencia |

## ğŸ® Uso
 (Recomendado)

```powershell
python app.py
```

La API iniciarÃ¡ en `http://localhost:5000` con todos los microservicios activos.

### OpciÃ³n 2: Usar los microservicios directamente

```python
from services import ScraperService, StorageService, SchedulerService
from config import Config

# Inicializar servicios
scraper = ScraperService()
storage = StorageService()
scheduler = SchedulerService(scraper, storage)

# Ejecutar scraping
data = scraper.scrape_homepage()

# Guardar datos
storage.save_json(data)
storage.save_csv(data)

# Iniciar scraping automÃ¡tico
scheduler.start(interval_hours=1)
```

### OpciÃ³n 3: Ejecutar pruebas

```powershell
# Prueba del scraper legacy
python test_scraper.py

# Prueba de microservicios
python test_microservices.py
```

### OpciÃ³n 4: Usar la API con curl

```bash
# Obtener informaciÃ³n
curl http://localhost:5000/

# Ejecutar scraping
curl http://localhost:5000/api/scrape

# Obtener datos
curl http://localhost:5000/api/data

# Ver estado del sistema
curl http://localhost:5000/api/status

# Configurar intervalo (2 horas)
curl -X POST http://localhost:5000/api/schedule \
  -H "Content-Type: applica para API REST
- **BeautifulSoup4** - Parsing HTML
- **Requests** - Peticiones HTTP
- **APScheduler** - Tareas programadas
- **Flask-CORS** - Cross-Origin Resource Sharing

## ğŸ¯ Ventajas de la Arquitectura de Microservicios

âœ… **SeparaciÃ³n de responsabilidades** - Cada servicio tiene una funciÃ³n especÃ­fica  
âœ… **Mantenibilidad** - CÃ³digo organizado y fÃ¡cil de modificar  
âœ… **Testabilidad** - Cada servicio puede probarse independientemente  
âœ… **Escalabilidad** - Los servicios pueden crecer o separarse en procesos independientes  
âœ… **ReutilizaciÃ³n** - Los servicios pueden usarse en otros proyectos  
âœ… **ConfiguraciÃ³n centralizada** - Un solo lugar para toda la configuraciÃ³n
## ğŸ“ Estructura de Datos

El scraper extrae y devuelve:

```json
{
  "timestamp": "2025-12-15T20:12:04.664509",
  "url": "https://www.inegi.org.mx",
  "title": "Instituto Nacional de EstadÃ­stica y GeografÃ­a (INEGI)",
  "status": "success",
  "main_sections": [],
  "latest_news": [],
  "featured_indicators": [],
  "links": []
}
```

## ğŸ“‚ Archivos Generados

Los datos se guardan en la carpeta `data/`:
- `inegi_data.json` - Datos en formato JSON
- `inegi_data.csv` - Datos en formato CSV
- `inegi_latest.json` - Ãšltima ejecuciÃ³n programada

## ğŸ› ï¸ TecnologÃ­as

- **Python 3.12**
- **Flask** - Framework web
- **BeautifulSoup4** - Parsing HTML
- **Requests** - Peticiones HTTP
- **APScheduler** - Tareas programadas
- **Flask-CORS** - Cross-Origin Resource Sharing

## âš™ï¸ ConfiguraciÃ³n

La API estÃ¡ configurada en `config.py`:
- â° **Cron Job automÃ¡tico cada 5 minutos** (configurable)
- Timeout de conexiÃ³n: **30 segundos**
- Puerto: **5000**
- Host: **0.0.0.0** (accesible desde la red local)

### Cambiar el intervalo del Cron Job:

1. **Editar config.py**:
```python
SCRAPING_INTERVAL_MINUTES = 10  # Cambiar a 10 minutos
```

2. **O usar la API**:
```powershell
# Cambiar a 10 minutos
Invoke-RestMethod -Uri "http://localhost:5000/api/schedule" -Method POST -Body '{"interval_minutes": 10}' -ContentType "application/json"
```

## ğŸ“ Notas

- El scraper respeta el sitio web usando User-Agent apropiado
- Los datos extraÃ­dos dependen de la estructura actual del sitio del INEGI
- Si el sitio cambia su estructura, puede ser necesario ajustar los selectores

## ğŸ› SoluciÃ³n de Problemas

**Error: ModuleNotFoundError**
```powershell
pip install -r requirements.txt
```

**Error: Connection refused**
- Verifica tu conexiÃ³n a internet
- El sitio del INEGI puede estar temporalmente no disponible

**La API no responde**
- Verifica que no haya otra aplicaciÃ³n usando el puerto 5000
- Revisa los logs en la terminal

## ğŸ“„ Licencia

Proyecto educativo/personal. Respeta los tÃ©rminos de uso del sitio del INEGI.
